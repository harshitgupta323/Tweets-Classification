{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8364bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import nltk\n",
    "from __future__ import division\n",
    "import math\n",
    "from sklearn.svm import LinearSVC\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import os\n",
    "#handle overfitting\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding,Dropout, Bidirectional\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a586ebb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"@KeyshawnSwag: Lmfao this cat started beating...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @digaveliavelife: Lol I be eatin da shit ou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @QueenReenie_: How bitch how? &amp;#8220;@_Vont...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet  id\n",
       "0      1  \"@KeyshawnSwag: Lmfao this cat started beating...   0\n",
       "1      1  RT @digaveliavelife: Lol I be eatin da shit ou...   1\n",
       "2      1  RT @QueenReenie_: How bitch how? &#8220;@_Vont...   2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:/downloads_14/H3_Multiclass_Hate_Speech_Detection_train.csv')\n",
    "df.head(3)\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e984b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/HP/Downloads/H3_Multiclass_Hate_Speech_Detection_test.csv\") #,encoding='unicode_escape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aeb3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(in_str):\n",
    "    in_str = str(in_str)\n",
    "    # replace urls with 'url'\n",
    "    in_str = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", in_str)\n",
    "    in_str = re.sub(r'([^\\s\\w]|_)+', '', in_str)\n",
    "    return in_str.strip().lower()\n",
    "\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(clean_str)\n",
    "test['tweet'] = test['tweet'].apply(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a13a85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    15398\n",
       "2     3291\n",
       "0     1137\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8ee39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df[df['label'] == 0].sample(frac=1)\n",
    "df_1 = df[df['label'] == 1].sample(frac=1)\n",
    "df_2 = df[df['label'] == 2].sample(frac=1)\n",
    "sample_size = 1137\n",
    "data = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_2.head(sample_size)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8b557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean length of sentence: 14.101436528877162\n",
      "max length of sentence: 32\n",
      "std dev length of sentence: 6.821066706531941\n"
     ]
    }
   ],
   "source": [
    "data['l'] = data['tweet'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(data.l.mean()))\n",
    "print(\"max length of sentence: \" + str(data.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(data.l.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4901a9",
   "metadata": {},
   "source": [
    "## LR with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d870c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = data['tweet'].values\n",
    "label = data['label'].values\n",
    "review_train, review_test, label_train, label_test = train_test_split(review, label, test_size=0.25, random_state=1000)\n",
    "\n",
    "review_vectorizer = CountVectorizer()\n",
    "review_vectorizer.fit(review_train)\n",
    "Xlr_train = review_vectorizer.transform(review_train)\n",
    "Xlr_test  = review_vectorizer.transform(review_test)\n",
    "\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel.fit(Xlr_train, label_train)\n",
    "score = LRmodel.score(Xlr_test, label_test)\n",
    "print(\"Accuracy:\", score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010b70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de9b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2767df5",
   "metadata": {},
   "source": [
    "## LR with Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4532fd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4957x12318 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 64793 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(train.tweet).toarray()\n",
    "labels = train.label\n",
    "features.shape\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel.fit(features, labels)\n",
    "\n",
    "# # score = LRmodel.score(Xlr_test, label_test)\n",
    "# # print(\"Accuracy:\", score) \n",
    "features_test = tfidf.transform(test.tweet)\n",
    "features_test.shape\n",
    "pred = LRmodel.predict(features_test)\n",
    "submission=test\n",
    "submission['label']=pred\n",
    "# submission.drop(['', 'Phrase'], axis=1, inplace=True)\n",
    "# submission.to_csv(\"submission.csv\", index = False)\n",
    "submission.to_csv(\"D:/res5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c94ae23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d61c7d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(Xlr_train, label_train)  #model\n",
    "y_predict = nb_tfidf.predict(Xlr_test)\n",
    "print(classification_report(label_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b85de9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Best parameters for NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f50ede3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Accuracy Through Grid Search : 0.738\n",
      "Best Parameters :  {'alpha': 10}\n"
     ]
    }
   ],
   "source": [
    "params = {'alpha': [0.01,0.1,0.5,1,10],\n",
    "         }\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "multinomial_nb_grid.fit(Xlr_train, label_train)\n",
    "y_predict = multinomial_nb_grid.predict(Xlr_test)\n",
    "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
    "print('Best Parameters : ',multinomial_nb_grid.best_params_)\n",
    "print(classification_report(label_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "84b1c7bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       277\n",
      "           1       0.76      0.73      0.74       301\n",
      "           2       0.83      0.85      0.84       275\n",
      "\n",
      "    accuracy                           0.75       853\n",
      "   macro avg       0.75      0.76      0.76       853\n",
      "weighted avg       0.75      0.75      0.75       853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_tfidf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "nb_tfidf.fit(Xlr_train, label_train)  #model\n",
    "y_predict = nb_tfidf.predict(Xlr_test)\n",
    "print(classification_report(label_test,y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9acf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "91eeda51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       296\n",
      "           1       0.00      0.00      0.00       284\n",
      "           2       0.32      1.00      0.48       273\n",
      "\n",
      "    accuracy                           0.32       853\n",
      "   macro avg       0.11      0.33      0.16       853\n",
      "weighted avg       0.10      0.32      0.16       853\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nb_tfidf = svm.SVC(kernel='rbf', C=1,gamma='auto')\n",
    "nb_tfidf.fit(Xlr_train, label_train)  #modelb\n",
    "y_predict = nb_tfidf.predict(Xlr_test)\n",
    "print(classification_report(label_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39aee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVC with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(train.tweet).toarray()\n",
    "labels = train.label\n",
    "features.shape\n",
    "model = LinearSVC()\n",
    "model.fit(features, labels)\n",
    "features_test = tfidf.transform(test.tweet)\n",
    "features_test.shape\n",
    "pred = model.predict(features_test)\n",
    "submission=test\n",
    "submission['label']=pred\n",
    "# submission.drop(['', 'Phrase'], axis=1, inplace=True)\n",
    "# submission.to_csv(\"submission.csv\", index = False)\n",
    "submission.to_csv(\"D:/res3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53550131",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d194fea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       277\n",
      "           1       0.76      0.73      0.74       301\n",
      "           2       0.83      0.85      0.84       275\n",
      "\n",
      "    accuracy                           0.75       853\n",
      "   macro avg       0.75      0.76      0.76       853\n",
      "weighted avg       0.75      0.75      0.75       853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gboost_m = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "train_pred = gboost_m.fit(Xlr_train, label_train).predict(Xlr_test)\n",
    "print(classification_report(label_test,y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6c177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 32\n",
    "max_features = 20000 # this is the number of words we care about\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "tokenizer.fit_on_texts(data['tweet'].values)\n",
    "\n",
    "# this takes our sentences and replaces each word with an integer\n",
    "X = tokenizer.texts_to_sequences(data['tweet'].values)\n",
    "x_test = tokenizer.texts_to_sequences(test['tweet'].values)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "x_test = pad_sequences(x_test, sequence_length)\n",
    "y = pd.get_dummies(data['label']).values\n",
    "\n",
    "# where there isn't a test set, Kim keeps back 10% of the data for testing, I'm going to do the same since we have an ok amount to play with\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(\"test set size \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db33b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909941ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the BASELINE MODEL\n",
    "base_model = Sequential()\n",
    "base_model.add(Embedding(num_words,128,input_length=X_train.shape[1]))\n",
    "base_model.add(GlobalAveragePooling1D())\n",
    "base_model.add(Dense(8,activation='relu'))\n",
    "base_model.add(Dense(3,activation='softmax'))\n",
    "base_model.summary()\n",
    "base_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history_base = base_model.fit(X_train, y_train ,epochs=20, validation_split=0.2)\n",
    "y_hat_5 = base_model.predict(X_test)\n",
    "loss, accuracy = base_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "predictions = base_model.predict(x_test)\n",
    "y_prob = base_model.predict(x_test)\n",
    "y_classes = y_prob.argmax(axis=-1) \n",
    "test['label'] = y_classes\n",
    "test.to_csv(\"D:/cnn3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc77d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e37a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34232104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1: Random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4737516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300 # Kim uses 300 here\n",
    "num_filters = 100\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# use a random embedding for the text\n",
    "embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "\n",
    "reshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n",
    "\n",
    "# Note the relu activation which Kim specifically mentions\n",
    "# He also uses an l2 constraint of 3\n",
    "# Also, note that the convolution window acts on the whole 200 dimensions - that's important\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "\n",
    "# perform max pooling on each of the convoluations\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "# concat and flatten\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "# do dropout and predict\n",
    "dropout = Dropout(0.5)(flatten)\n",
    "output = Dense(units=3, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 32 # Kim uses 50 here, I have a slightly smaller sample size than num\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=batch_size, verbose=1, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "tokenizer.fit_on_texts(test['tweet'].values)\n",
    "# this takes our sentences and replaces each word with an integer\n",
    "X = tokenizer.texts_to_sequences(test['tweet'].values)\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "y_prob = model.predict(X)\n",
    "y_classes = y_prob.argmax(axis=-1) \n",
    "test['label'] = y_classes\n",
    "test.to_csv(\"D:/cnn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0451152",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)\n",
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))\n",
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b3f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15159c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87512e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aafecc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2: Static word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc4f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('D:/', 'glove.42B.300d.txt'),encoding='utf-8') #glove.42B.300d.txt\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "embedding_dim = 300 # Kim uses 300 here\n",
    "num_filters = 100\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b593f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c92c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73b8f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2 = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# note the `trainable=False`, later we will make this layer trainable\n",
    "embedding_layer_2 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=False)(inputs_2)\n",
    "\n",
    "reshape_2 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_2)\n",
    "\n",
    "conv_0_2 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "conv_1_2 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "conv_2_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "\n",
    "maxpool_0_2 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_2)\n",
    "maxpool_1_2 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_2)\n",
    "maxpool_2_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_2)\n",
    "\n",
    "concatenated_tensor_2 = Concatenate(axis=1)([maxpool_0_2, maxpool_1_2, maxpool_2_2])\n",
    "flatten_2 = Flatten()(concatenated_tensor_2)\n",
    "\n",
    "dropout_2 = Dropout(0.5)(flatten_2)\n",
    "output_2 = Dense(units=3, activation='softmax')(dropout_2)\n",
    "\n",
    "model_2 = Model(inputs=inputs_2, outputs=output_2)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_2.summary())\n",
    "\n",
    "batch_size = 32\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=20, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca253759",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_2 = model_2.predict(X_test)\n",
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))\n",
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))\n",
    "loss, accuracy = model_2.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad816519",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_2.predict(x_test)\n",
    "y_prob = model_2.predict(x_test)\n",
    "y_classes = y_prob.argmax(axis=-1) \n",
    "test['label'] = y_classes\n",
    "test.to_csv(\"D:/cnn1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6f6f8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history_2.history['accuracy'])\n",
    "# plt.plot(history_2.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(history_2.history['loss'])\n",
    "# plt.plot(history_2.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a35bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04329546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3: w2v with trainable embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d576313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_3 = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding_layer_3 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(inputs_3)\n",
    "\n",
    "reshape_3 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n",
    "\n",
    "# note the relu activation\n",
    "conv_0_3 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_1_3 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_2_3 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "\n",
    "maxpool_0_3 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\n",
    "maxpool_1_3 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\n",
    "maxpool_2_3 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n",
    "\n",
    "concatenated_tensor_3 = Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\n",
    "flatten_3 = Flatten()(concatenated_tensor_3)\n",
    "\n",
    "dropout_3 = Dropout(0.5)(flatten_3)\n",
    "output_3 = Dense(units=3, activation='softmax')(dropout_3)\n",
    "\n",
    "model_3 = Model(inputs=inputs_3, outputs=output_3)\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_3.summary())\n",
    "\n",
    "batch_size = 32\n",
    "history_3 = model_3.fit(X_train, y_train, epochs=20, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b38f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_3 = model_3.predict(X_test)\n",
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))\n",
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))\n",
    "loss, accuracy = model_3.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_3.predict(x_test)\n",
    "y_prob = model_3.predict(x_test)\n",
    "y_classes = y_prob.argmax(axis=-1) \n",
    "test['label'] = y_classes\n",
    "test.to_csv(\"D:/cnn2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "efe71e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history_3.history['accuracy'])\n",
    "# plt.plot(history_3.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(history_3.history['loss'])\n",
    "# plt.plot(history_3.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e1b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CNN random       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))))\n",
    "print(\"CNN static       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))))\n",
    "print(\"CNN trainable    : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))))\n",
    "# LR = 79\n",
    "# NB = 75\n",
    "# DT = 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bae988ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c6ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10398 unique tokens.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 128)           1331072   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 128)           0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 1032      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 27        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,332,131\n",
      "Trainable params: 1,332,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Removing overfitiing\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "embedding_dim = 300 # Kim uses 300 here\n",
    "num_filters = 100\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "regularise = tf.keras.regularizers.l2(0.001)\n",
    "\n",
    "model_r = Sequential()\n",
    "model_r.add(Embedding(num_words,128,input_length=X_train.shape[1]))\n",
    "model_r.add(Dropout(0.5))\n",
    "model_r.add(GlobalAveragePooling1D())\n",
    "model_r.add(Dense(8,activation='relu',kernel_regularizer=regularise))\n",
    "model_r.add(Dropout(0.5))\n",
    "model_r.add(Dense(3,activation='softmax'))\n",
    "model_r.summary()\n",
    "\n",
    "#Compiling the model\n",
    "model_r.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history_r = model_r.fit(X_train, y_train ,epochs=20, validation_split=0.2)\n",
    "y_hat_6 = model_r.predict(X_test)\n",
    "\n",
    "loss, accuracy = model_r.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "\n",
    "predictions = model_r.predict(x_test)\n",
    "y_prob = model_r.predict(x_test)\n",
    "y_classes = y_prob.argmax(axis=-1) \n",
    "test['label'] = y_classes\n",
    "test.to_csv(\"D:/cnn4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6922baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18790df7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "77/77 [==============================] - 4s 26ms/step - loss: 1.1038 - accuracy: 0.4395 - val_loss: 1.0286 - val_accuracy: 0.5961\n",
      "Epoch 2/20\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.8073 - accuracy: 0.6497 - val_loss: 0.7595 - val_accuracy: 0.6906\n",
      "Epoch 3/20\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.4939 - accuracy: 0.8102 - val_loss: 0.7252 - val_accuracy: 0.7524\n",
      "Epoch 4/20\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.3238 - accuracy: 0.8957 - val_loss: 0.8476 - val_accuracy: 0.6971\n",
      "Epoch 5/20\n",
      "77/77 [==============================] - 2s 20ms/step - loss: 0.1909 - accuracy: 0.9479 - val_loss: 0.9519 - val_accuracy: 0.7150\n",
      "Epoch 6/20\n",
      "77/77 [==============================] - 2s 20ms/step - loss: 0.1327 - accuracy: 0.9617 - val_loss: 1.1311 - val_accuracy: 0.6954\n",
      "11/11 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "#Building the model\n",
    "from keras.constraints import max_norm\n",
    "model1 = Sequential([\n",
    "    Embedding(num_words,128,input_length=X_train.shape[1]),\n",
    "    Dropout(0.5),\n",
    "    LSTM(32,kernel_constraint=max_norm(3)),\n",
    "    Dense(32,activation='relu',kernel_regularizer=regularise),\n",
    "    Dropout(0.5),\n",
    "    Dense(3,activation='softmax')\n",
    "])\n",
    "#Compiling the model\n",
    "model1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#Fitting the model\n",
    "history1 =  model1.fit(X_train, y_train ,epochs=20, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=3)])\n",
    "y_hat_7 = model1.predict(X_test)\n",
    "\n",
    "loss, accuracy = model1.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "predictions = model1.predict(x_test)\n",
    "y_prob = model1.predict(x_test)\n",
    "y_classes = y_prob.argmax(axis=-1) \n",
    "test['label'] = y_classes\n",
    "test.to_csv(\"D:/cnn5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "948e4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "77/77 [==============================] - 13s 80ms/step - loss: 1.0574 - accuracy: 0.4033 - val_loss: 0.9574 - val_accuracy: 0.6124\n",
      "Epoch 2/20\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.6438 - accuracy: 0.7426 - val_loss: 0.6321 - val_accuracy: 0.7443\n",
      "Epoch 3/20\n",
      "77/77 [==============================] - 4s 57ms/step - loss: 0.3301 - accuracy: 0.9018 - val_loss: 0.7588 - val_accuracy: 0.7313\n",
      "Epoch 4/20\n",
      "77/77 [==============================] - 4s 57ms/step - loss: 0.1588 - accuracy: 0.9597 - val_loss: 1.1014 - val_accuracy: 0.7166\n",
      "Epoch 5/20\n",
      "77/77 [==============================] - 4s 57ms/step - loss: 0.0694 - accuracy: 0.9845 - val_loss: 1.6968 - val_accuracy: 0.7101\n",
      "11/11 [==============================] - 2s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "#Building the model\n",
    "model2 = Sequential([\n",
    "    Embedding(num_words,128,input_length=X_train.shape[1]),\n",
    "    Bidirectional(LSTM(128,return_sequences=True)),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3,activation='softmax')\n",
    "])\n",
    "#Compiling the model\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#Fitting the model\n",
    "history2 =  model2.fit(X_train, y_train ,epochs=20, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=3)])\n",
    "y_hat_8 = model2.predict(X_test)\n",
    "loss, accuracy = model2.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "predictions = model2.predict(x_test)\n",
    "y_prob = model2.predict(x_test)\n",
    "y_classes = y_prob.argmax(axis=-1) \n",
    "test['label'] = y_classes\n",
    "test.to_csv(\"D:/cnn6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "68ba8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
